-----------------------------------------------------------------
Explanation of the Directory Structure
-----------------------------------------------------------------

The POS Tagger folder contains the following folders:
* CrossValidation tagger models: Contains the tagger models created to compute the precision, recall and f-measure of the final trained model by cross-validation
* Tagged Testing Data: Contains the testing data tagged using the final trained model
* Tagged Training Set: Contains the manually corrected tagged training subsets
* Tagged using Final Tagger: Contains the complete training data tagged using the final trained model
* Training POS Models: Contains the intermediate tagger models and final tagger model created during the training process
* Untagged Testing Data: Contains the the untagged test set (plain text tweets)
* Untagged Training Data: Contains the untagged training set (plain text tweets)

The Parser folder contains the following folders and files:
* caseless_default_parser: Contains the final output for each subset generated by the default caseless variation of the Stanford parser 
* cross_validation_models: Contains the parser models created to compute the precision, recall and f-measure of the final trained model by cross-validation
* cross_validation_output: Contains the parser output generated by each model to compute the precision, recall and f-measure of the final trained model by cross-validation
* output_from_parser: Contains the output that was generated at each stage of training
* serialized_models: Contains the parser models that were generated at each stage of training 
* tagged_sets: Contains the 10 sets of tweets that have been tagged using the final POS tagger, to be used for training the parser
* testing: Contains the testing data parsed using the final trained model
* training_for_parser: Contains the manually corrected parsed tweets for each subset
* untagged_sets: Contains the untagged version of each subset along with the cumulative subsets (plain text tweets)
* unused_parsed_output: Contains the output of each parser model on the subsets that weren't used for training the parser
* unused_tagged_sets: Contains tagged sets that weren't used for training the parser (plain text tweets)
* unused_untagged_sets: Contains the untagged version of each subset that wasn't used for training the parser (plain text tweets)

-----------------------------------------------------------------
Corpus
-----------------------------------------------------------------

Run the following command on the command line:
	./main.sh

This will use the plaintext tweets, normalise them, tag them using the TwitIE POS tagger, and parse them using the Stanford NLP Parser.


-----------------------------------------------------------------
POS TAGGER
-----------------------------------------------------------------

SampleSet1.txt was initially tagged using the default model gate-EN-twitter.model provided with the TwitIE Part of Speech tagger.
The following command was used to store the tagged file as Set1_output.txt:

 java - jar twitie_tag.jar Training\ POS\ Models/gate-EN-twitter.model SampleSet1.txt > Set1_output.txt

Set1_output.txt was manually corrected and stored in the folder Tagged Training Set. This corrected to file was then used to train a new tagger model gago.tagger, using the following command:

java -cp stanford-postagger.jar -mx1g edu.stanford.nlp.tagger.maxent.MaxentTagger -props default.tagger.props

NOTE: The TwitIE tagger makes use of Stanford's MaxentTagger and since a training method could not be found for the TwitIE tagger, the Stanford training method was used.

The default.tagger.props train file included Set1_corrected.txt and gago.tagger model was generated. This model was then used to tag SampleSet2.txt. The generic command is as follows:

java -jar twitie_tag.jar <path to model file>  <path to input file>  >  <output file>

The above steps were then repeated until all 10 sample sets of the training data were corrected. Subsequently, the 10 corrected files were used to train the final model, finalpos.tagger. The final trained model was applied to the training data and stored in the folder Tagged using Final Tagger.

java -jar twitie_tag.jar Training\ POS\ Models/finalpos.tagger Untagged\ Training\ Data/SampleSet1.txt   >  Tagged\ Using\ Final\ Tagger/Set1_final.txt

NOTE: 
There is a known "InvocationTargetException" problem when using Apple's own Java distribution with TwitIE; using the OpenJDK JRE or Java 1.7 can remedy this. Sometimes, the jar file is required to be placed in the same folder as that of the model being used.

-----------------------------------------------------------------
PARSER
-----------------------------------------------------------------

SampleSet1.txt was initially parsed using the default Caseless Probablistic Context Free Grammar Model englishPCFG.caseless.ser.gz, a variation of the Stanford Parser.

The following command was used to store the parsed file as output_1.txt:

java -cp "*" -mx1g edu.stanford.nlp.parser.lexparser.LexicalizedParser -sentences newline -tokenized -tagSeparator _ -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerMethod newCoreLabelTokenizerFactory edu/stanford/nlp/models/lexparser/englishPCFG.caseless.ser.gz "Parser/tagged_sets/SampleSet1.txt" > "Parser/output_from_parser/output_1.txt"

output_1.txt was manually corrected for parsing errors and stored in the training_for_parser folder as train1.txt. This trained file was then used to train a new parser model SerializedModel1, using the following command:

java -cp "*" -mx1g edu.stanford.nlp.parser.lexparser.LexicalizedParser -PCFG -headFinder edu.stanford.nlp.trees.LeftHeadFinder -sentences newline -train "Parser/training_for_parser/train1.txt" 1 -saveToSerializedFile "Parser/serialized_models/SerializedModel1" -writeOutputFiles "Parser/untagged_sets/SampleSet1.txt"

The model SerializedModel1 was generated. This model was then used to parser SampleSet1.txt. The generic commands for the subsequent steps is as follows:

java -cp "*" -mx1g edu.stanford.nlp.parser.lexparser.LexicalizedParser -sentences newline -tokenized -tagSeparator _ -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerMethod newCoreLabelTokenizerFactory -loadFromSerializedFile "<path to model file>" "<path to tagged file>" > <ouput file>

java -cp "*" -mx1g edu.stanford.nlp.parser.lexparser.LexicalizedParser -PCFG -headFinder edu.stanford.nlp.trees.LeftHeadFinder -sentences newline -train "<path to training data>" 1-<number of current set> -saveToSerializedFile "<path to model>" -writeOutputFiles "<path to untaged set>"

The above steps are repeated until all 10 sample sets of 500+ tokens of the training data are corrected. Subsequently, the 10 corrected files were used to train the final model, SerializedModel10. 

	



